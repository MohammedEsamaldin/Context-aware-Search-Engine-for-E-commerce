{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.0.1-py3-none-any.whl (340 kB)\n",
      "\u001b[K     |████████████████████████████████| 340 kB 12.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting annoy\n",
      "  Downloading annoy-1.17.3.tar.gz (647 kB)\n",
      "\u001b[K     |████████████████████████████████| 647 kB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.5.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Downloading transformers-4.50.2-py3-none-any.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 9.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.11.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from sentence-transformers) (2.6.0)\n",
      "Collecting huggingface-hub>=0.20.0\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "\u001b[K     |████████████████████████████████| 468 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.9 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: filelock in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "\u001b[K     |████████████████████████████████| 418 kB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.7 MB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 18.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 14.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Building wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.3-cp39-cp39-macosx_10_12_universal2.whl size=116251 sha256=82b617ccd1bba8c2c9d8dd20872385fafcd8933ca1bd59e9e3698d823751ff92\n",
      "  Stored in directory: /Users/mohammed/Library/Caches/pip/wheels/09/a9/54/37478e65995fe712f7da465749da9ddb21db6b1a599d591ac7\n",
      "Successfully built annoy\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, requests, pyyaml, huggingface-hub, tokenizers, safetensors, transformers, sentence-transformers, annoy\n",
      "Successfully installed annoy-1.17.3 certifi-2025.1.31 charset-normalizer-3.4.1 huggingface-hub-0.29.3 idna-3.10 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 sentence-transformers-4.0.1 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.50.2 urllib3-2.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers annoy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<annoy.Annoy object at 0x13e44b410>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Query: who is Mohammed \n",
      "\n",
      "Most similar documents:\n",
      "Document: this document is talking about how Mohammed is awsome, and cool. (Distance: 0.7285750508308411)\n",
      "Document: this document is mentioning that mohammed does want to get a sleep (Distance: 0.8530937433242798)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize Sentence-BERT model for embedding generation\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Example model, can be changed\n",
    "\n",
    "# Sample documents and a refined query\n",
    "documents = [\n",
    "            \"This is the first document about artificial intelligence.\",\n",
    "            \"Machine learning is a subset of artificial intelligence.\",\n",
    "            \"Deep learning models are powerful for many tasks.\",\n",
    "            \"Natural language processing focuses on the interaction between computers and human languages.\",\n",
    "            \"Reinforcement learning is a type of machine learning where agents learn to make decisions.\",\n",
    "            \"this document is talking about how Mohammed is awsome, and cool.\",\n",
    "            \"this document is mentioning that mohammed does want to get a sleep\"\n",
    "        ]\n",
    "\n",
    "refined_query = str(input(\"please enter you query?\"))\n",
    "\n",
    "# Step 2: Create vectors for documents and the refined query\n",
    "document_vectors = model.encode(documents)  # Get embeddings for documents\n",
    "\n",
    "query_vector = model.encode([refined_query])[0]  # Get embedding for the query\n",
    "\n",
    "# Step 3: Initialize Annoy index\n",
    "vector_dimension = len(document_vectors[0])  # Dimension of the vector\n",
    "annoy_index = AnnoyIndex(vector_dimension, 'angular')  # Use angular distance (cosine similarity)\n",
    "\n",
    "\n",
    "# Step 4: Add document vectors to Annoy index\n",
    "for i, vector in enumerate(document_vectors):\n",
    "    annoy_index.add_item(i, vector)\n",
    "\n",
    "# Build the Annoy index (this is an offline process)\n",
    "annoy_index.build(10)  # 10 trees for speed/accuracy trade-off\n",
    "print(annoy_index)\n",
    "print(100*\"-\")\n",
    "\n",
    "# Step 5: Query the Annoy index to find the most similar document\n",
    "nearest_neighbors = annoy_index.get_nns_by_vector(query_vector, 2, include_distances=True)\n",
    "\n",
    "# Step 6: Output the results\n",
    "print(f\"Query: {refined_query}\")\n",
    "print(\"\\nMost similar documents:\")\n",
    "for idx, dist in zip(nearest_neighbors[0], nearest_neighbors[1]):\n",
    "    print(f\"Document: {documents[idx]} (Distance: {dist})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".........F\n",
      "======================================================================\n",
      "FAIL: test_similarity_search (__main__.TestAnnoySearchSystem)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/yp/tlgq8_fj0418jxt8_9b3rw_80000gn/T/ipykernel_33939/566361203.py\", line 60, in test_similarity_search\n",
      "    self.assertEqual(self.documents[closest_doc_idx], expected_document)\n",
      "AssertionError: 'this document is talking about how Mohamm[19 chars]ool.' != 'Reinforcement learning is a type of machi[45 chars]ons.'\n",
      "- this document is talking about how Mohammed is awsome, and cool.\n",
      "+ Reinforcement learning is a type of machine learning where agents learn to make decisions.\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 9.809s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "class TestAnnoySearchSystem(unittest.TestCase):\n",
    "    \n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Set up the model and sample documents\n",
    "        cls.model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        cls.documents = [\n",
    "            \"This is the first document about artificial intelligence.\",\n",
    "            \"Machine learning is a subset of artificial intelligence.\",\n",
    "            \"Deep learning models are powerful for many tasks.\",\n",
    "            \"Natural language processing focuses on the interaction between computers and human languages.\",\n",
    "            \"Reinforcement learning is a type of machine learning where agents learn to make decisions.\",\n",
    "            \"this document is talking about how Mohammed is awsome, and cool.\",\n",
    "            \"this document is mentioning that mohammed does want to get a sleep\"\n",
    "        ]\n",
    "        cls.refined_query = \"\"\n",
    "\n",
    "        # Encode documents and the query\n",
    "        cls.document_vectors = cls.model.encode(cls.documents)\n",
    "        cls.query_vector = cls.model.encode([cls.refined_query])[0]\n",
    "\n",
    "        # Initialize Annoy index\n",
    "        cls.vector_dimension = len(cls.document_vectors[0])\n",
    "        cls.annoy_index = AnnoyIndex(cls.vector_dimension, 'angular')\n",
    "\n",
    "        # Add document vectors to Annoy index\n",
    "        for i, vector in enumerate(cls.document_vectors):\n",
    "            cls.annoy_index.add_item(i, vector)\n",
    "\n",
    "        # Build the index\n",
    "        cls.annoy_index.build(10)\n",
    "\n",
    "    def test_document_embedding_generation(self):\n",
    "        # Test that document vectors are generated properly\n",
    "        self.assertEqual(len(self.document_vectors), len(self.documents))\n",
    "        self.assertEqual(len(self.document_vectors[0]), self.vector_dimension)\n",
    "    \n",
    "    def test_query_embedding_generation(self):\n",
    "        # Test that query vector is generated properly\n",
    "        self.assertEqual(len(self.query_vector), self.vector_dimension)\n",
    "\n",
    "    def test_annoy_index_creation(self):\n",
    "        # Test that the Annoy index is built correctly by checking the number of items\n",
    "        self.assertEqual(self.annoy_index.get_n_items(), len(self.documents))\n",
    "\n",
    "    def test_similarity_search(self):\n",
    "        # Perform a similarity search for the query and check if results make sense\n",
    "        nearest_neighbors = self.annoy_index.get_nns_by_vector(self.query_vector, 3, include_distances=True)\n",
    "        \n",
    "        # Assert that we get exactly 3 nearest neighbors\n",
    "        self.assertEqual(len(nearest_neighbors[0]), 3)\n",
    "\n",
    "        # Check that the first result is the most similar document (should be related to reinforcement learning)\n",
    "        expected_document = \"Reinforcement learning is a type of machine learning where agents learn to make decisions.\"\n",
    "        closest_doc_idx = nearest_neighbors[0][0]\n",
    "        self.assertEqual(self.documents[closest_doc_idx], expected_document)\n",
    "\n",
    "    def test_cosine_similarity_behavior(self):\n",
    "        # Check that the cosine similarity is reasonable (distance should be small for similar documents)\n",
    "        nearest_neighbors = self.annoy_index.get_nns_by_vector(self.query_vector, 3, include_distances=True)\n",
    "        \n",
    "        for dist in nearest_neighbors[1]:\n",
    "            # Cosine distance should be between 0 (similar) and 2 (opposite direction)\n",
    "            self.assertGreaterEqual(dist, 0)\n",
    "            self.assertLessEqual(dist, 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main(argv=[''], exit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[K     |████████████████████████████████| 509 kB 14.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /Users/mohammed/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Installing collected packages: tzdata, pytz, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
